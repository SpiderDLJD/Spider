
# Spider

# coding=utf-8

import requests

from lxml import etree

import xlwt

import time



# 初始化爬虫数据

request_url = "https://search.bilibili.com/all?keyword=Python&from_source=banner_search&spm_id_from=333.334.banner_link.1&page={}"

headers = {

    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',

    'Accept-Encoding': 'gzip, deflate, br',

    'Accept-Language': 'zh-CN,zh;q=0.9',

    'Cache-Control': 'max-age=0',

    'Connection': 'keep-alive',

    'Cookie': 'finger=edc6ecda; buvid3=A3A829DB-C0C6-4A4F-B7D5-2BD1407EE7F06721infoc; LIVE_BUVID=AUTO5315316677538879; fts=1531668311; sid=i1gej45n',

    'Host': 'search.bilibili.com',

    'Referer': 'https://www.bilibili.com/',

    'Upgrade-Insecure-Requests': '1',

    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'

}



# 爬取1~50页数据

pages = range(1, 50)



# 初始化Excel

workbook = xlwt.Workbook(encoding='utf-8')

worksheet = workbook.add_sheet(u'所爬取数据', cell_overwrite_ok=True)

row = [u'编号', u'标题', u'视频链接', u'观看量', u'作者', u'上传时间']

j = 1

for i in range(len(row)):

    worksheet.write(0, i, row[i])



# xpath方法爬虫主程序

for page in pages:

    url = request_url.format(page)

    time.sleep(1)

    print(u'请等待1秒...')

    print(u'第%d页' % page)

    print(u'网址：%s' % url)

    response = requests.get(url=url, headers=headers)

    if response.status_code == 200:

        source = etree.HTML(response.text)

        divs = source.xpath('//*[@id="server-search-app"]/div[2]/div[2]/div/div[2]/ul/li')

        for div in divs:

            data = []

            title = div.xpath('./div/div[1]/a/@title')

            video_link = div.xpath('./div/div[1]/a/@href')

            views = div.xpath('./div/div[3]/span[1]/text()')

            author = div.xpath('./div/div[3]/span[4]/a/text()')

            create_time = div.xpath('./div/div[3]/span[3]/text()')



            # 判断所爬取的数据是否为空

            title = title[0].strip() if len(title) > 0 else ''

            video_link = video_link[0].replace('//', '') if len(video_link) > 0 else ''

            views = views[0].strip() if len(views) > 0 else ''

            author = author[0].strip() if len(author) > 0 else ''

            create_time = create_time[0].strip() if len(create_time) > 0 else ''



            # 将数据输入表格

            data.append(j)

            data.append(title)

            data.append(video_link)

            data.append(views)

            data.append(author)

            data.append(create_time)

            for i in range(len(data)):

                worksheet.write(j, i, data[i])

            j += 1



            # 打印数据

            print('爬取第%{}条数据'.format(j))

    else:

        print '\n\n出现错误,错误信息是:{}\n\n'.format(e.message)

workbook.save('bilibi_data_xpath.xls')

print(u'爬虫工作已完成')
